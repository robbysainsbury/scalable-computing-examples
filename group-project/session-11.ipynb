{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# For parallel processing\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.channels import LocalChannel\n",
    "from parsl.executors import HighThroughputExecutor\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "# helpers\n",
    "from grouputils import initialize_rasterizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "![](https://raw.githubusercontent.com/PermafrostDiscoveryGateway/viz-raster/develop/docs/images/raster_tldr.png)\n",
    "\n",
    "Today, we will create rasters from the regularly gridded staged data from yesterday (vector data, left side of diagram above). Each pixel of the raster will contain a stasticic calculated based on the underlying vector data for that pixel.\n",
    "\n",
    "The two statistics we calculate are:\n",
    "\n",
    "- number of IWP per pixel\n",
    "- proportion of pixel covered by IWP\n",
    "\n",
    "The vector data on the left are geopackages, which can be read in as simple geodataframes. The rasters take the geometry columns of all the geodataframes, which are all simple polygons, and represent them as pixels. \n",
    "\n",
    "Similar to the `TileStager` from the first step of the group project, in this step we will use a `RasterTiler` class, which we initialize using `initialize_rasterizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing raster\n",
    "iwp_rasterizer = initialize_rasterizer('/home/shares/example-pdg-data/SCC-2023')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to how the `iwp_stager` communicated configuration information to the stager, the `iwp_rasterizer` communicates  information to the rasterizer. This includes:\n",
    "\n",
    "- where the staged files are located\n",
    "- where to write the raster files\n",
    "- size of the rasters (in pixels)\n",
    "- what statistics to calculate (each becomes a band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's have a look at the data we need to rasterize first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "454\n"
     ]
    }
   ],
   "source": [
    "staged_paths = iwp_rasterizer.tiles.get_filenames_from_dir('staged')\n",
    "print(len(staged_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `rasterize_vector` method on our `RasterTiler` class object, and pass it the first file in our list of staged files to rasterize just one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tile(x=901, y=1060, z=13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwp_rasterizer.rasterize_vector(staged_paths[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This went pretty quickly, but with hundreds of files, it would still be beneficial to parallelize. Estimate the length of time it would take to process the files in series below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363.20000000000005"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate duration of rasterization process\n",
    ".8*454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rasterize in Parallel\n",
    "\n",
    "Given what you know about the limits of parallelization, discuss in your group whether you think this problem is:\n",
    "\n",
    "- cpu-bound\n",
    "- memory-bound\n",
    "- I/O-bound\n",
    "- network-bound\n",
    "\n",
    "How does it compare to the parallelization task from yesterday?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of parallelizing this step over all 454 files, we will instead process the data in batches of files. Below is a function we can use to make batches of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because rasterization is relatively quick, we want each parsl \"task\" to process a batch of tiles.\n",
    "def make_batch(items, batch_size):\n",
    "    # Create batches of a given size from a list of items.\n",
    "    return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `make_batch` function to make batches of 10 items each of our staged data (`staged_paths`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batches of 10 files\n",
    "batches = make_batch(staged_paths,10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set up your Parsl executor again, with `max_workers` set at 11 and `max_blocks` set at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7f82f72d9dc0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEMPLATE FOR PARSL CONFIG:\n",
    "\n",
    "# activate_env = 'workon scomp'\n",
    "# htex_config = Config(\n",
    "#   executors=[\n",
    "#       HighThroughputExecutor(\n",
    "#           ..., \n",
    "#           provider = LocalProvider(\n",
    "#             worker_init = activate_env,\n",
    "#             ...\n",
    "#           )\n",
    "#       )\n",
    "#   ]\n",
    "# )\n",
    "\n",
    "activate_env = 'workon scomp'\n",
    "htex_config = Config(\n",
    "  executors=[\n",
    "      HighThroughputExecutor(\n",
    "          max_workers = 11, \n",
    "          provider = LocalProvider(\n",
    "            worker_init = activate_env,\n",
    "            max_blocks = 1\n",
    "          )\n",
    "      )\n",
    "  ]\n",
    ")\n",
    "\n",
    "parsl.clear()\n",
    "parsl.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up your Parsl app to run the `rasterize_vector` method in parallel. Remember that Parsl apps cannot rely on global variables or package imports, so you'll need to make sure to pass the app all of the variables it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Parsl app that uses the rasterize_vectors method\n",
    "@python_app\n",
    "def rasterize_batch(batch, rasterizer):\n",
    "    for file in batch:\n",
    "        rasterizer.rasterize_vector(file)\n",
    "    \n",
    "    return 'done'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, execute the app in parallel over all of the batches of files you created previously.\n",
    "\n",
    "While you wait for _all_ the geotiffs to be written, you can monitor the process with the command: `find group-project/geotiff -type f | wc -l` \n",
    "\n",
    "The total number of geotiffs for this step will be equivalent to the number of staged files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done',\n",
       " 'done']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rasterize the batches in parallel\n",
    "app_futures = []\n",
    "for batch in batches:\n",
    "    app_future = rasterize_batch(batch, iwp_rasterizer)\n",
    "    app_futures.append(app_future)\n",
    "\n",
    "# Don't continue to print message until all tiles have been rasterized\n",
    "[app_future.result() for app_future in app_futures]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to add lines to shut down the executor and clear `parsl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'htex_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sainsbury/scalable-computing-examples/group-project/session-11.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bincluded-crab.nceas.ucsb.edu/home/sainsbury/scalable-computing-examples/group-project/session-11.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m htex_config\u001b[39m.\u001b[39mexecutors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshutdown()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bincluded-crab.nceas.ucsb.edu/home/sainsbury/scalable-computing-examples/group-project/session-11.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m parsl\u001b[39m.\u001b[39mclear()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'htex_config' is not defined"
     ]
    }
   ],
   "source": [
    "htex_config.executors[0].shutdown()\n",
    "parsl.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that you have the same number of GeoTIFF files as we do staged vector tiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotiff_paths = iwp_rasterizer.tiles.get_filenames_from_dir('geotiff')\n",
    "len(geotiff_paths) == len(staged_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a 1:1 ratio because we only processed the highest resolution files: those at zoom-level 13."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Thinking back to the group project yesterday, how would you test whether this problem is CPU or I/O bound? One option would be to test whether adding more CPU decreases your computation time. Set up a test on a subset of the staged files (the first 100 or so is fine) and time the process at different `max_workers`. To make sure you don't overload the server, do not exceed 15 workers in your tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('scomp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0030ed2ed4c0609037967981d5426019e14a913516344490dbc8ffbbe92f86b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
